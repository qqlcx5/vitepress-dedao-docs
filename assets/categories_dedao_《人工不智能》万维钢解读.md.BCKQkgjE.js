import{f as o,D as t,c as r,o as A,a1 as B,b as l,k as s,w as p,a,H as e,a2 as i}from"./chunks/framework.Baju3tsf.js";const C=JSON.parse('{"title":"《人工不智能》| 万维钢解读","description":"高估人工智能，会导致一系列的社会问题。","frontmatter":{"title":"《人工不智能》| 万维钢解读","description":"高估人工智能，会导致一系列的社会问题。","created":"2025-01-18T16:07:07.000Z","modification_date":"2025-01-18T16:07:07.000Z","tags":["科技/人工智能"],"cover_url":"https://piccdn3.umiwi.com/img/201902/13/201902131407541447429003.jpg?x-oss-process=image/resize,m_fill,h_224,w_168","source":"https://www.dedao.cn/audioBook/detail?id=ZV1po7jlgBdObqN5y0GNR4wLzxya5v&score=4.6&learn_count=12万次学习","author":null},"headers":[],"relativePath":"categories/dedao/《人工不智能》万维钢解读.md","filePath":"categories/dedao/《人工不智能》万维钢解读.md"}'),d={name:"categories/dedao/《人工不智能》万维钢解读.md"};function m(g,E,c,u,D,h){const n=t("Mermaid");return A(),r("div",null,[E[3]||(E[3]=B('<h1 id="《人工不智能》万维钢解读" tabindex="-1">《人工不智能》万维钢解读 <a class="header-anchor" href="#《人工不智能》万维钢解读" aria-label="Permalink to &quot;《人工不智能》万维钢解读&quot;">​</a></h1><h2 id="可视化总结" tabindex="-1">可视化总结 <a class="header-anchor" href="#可视化总结" aria-label="Permalink to &quot;可视化总结&quot;">​</a></h2><p>#科技/人工智能</p><h2 id="mermaid-流程图-教材发放流程" tabindex="-1">Mermaid 流程图：教材发放流程 <a class="header-anchor" href="#mermaid-流程图-教材发放流程" aria-label="Permalink to &quot;Mermaid 流程图：教材发放流程&quot;">​</a></h2>',4)),(A(),l(i,null,{default:p(()=>[e(n,{id:"mermaid-12",class:"mermaid",graph:"graph%20LR%0AA%5B%E7%BB%8F%E8%B4%B9%E7%94%B3%E8%AF%B7%5D%20--%3E%20B%7B%E7%BB%8F%E8%B4%B9%E6%98%AF%E5%90%A6%E5%88%B0%E4%BD%8D%3F%7D%0AB%20--%20%E5%88%B0%E4%BD%8D%20--%3E%20C%5B%E6%95%99%E6%9D%90%E9%87%87%E8%B4%AD%5D%0AB%20--%20%E6%9C%AA%E5%88%B0%E4%BD%8D%20--%3E%20D%5B%E7%AD%89%E5%BE%85%2F%E9%87%8D%E6%96%B0%E7%94%B3%E8%AF%B7%5D%0AC%20--%3E%20E%5B%E6%95%99%E6%9D%90%E5%85%A5%E5%BA%93%5D%0AE%20--%3E%20F%7B%E6%95%99%E6%9D%90%E6%95%B0%E9%87%8F%E4%B8%8E%E5%AD%A6%E7%94%9F%E6%95%B0%E9%87%8F%E6%98%AF%E5%90%A6%E5%8C%B9%E9%85%8D%3F%7D%0AF%20--%20%E5%8C%B9%E9%85%8D%20--%3E%20G%5B%E6%95%99%E6%9D%90%E5%8F%91%E6%94%BE%5D%0AF%20--%20%E4%B8%8D%E5%8C%B9%E9%85%8D%20--%3E%20H%5B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5%2F%E8%B0%83%E6%95%B4%5D%0AG%20--%3E%20I%5B%E5%AD%A6%E7%94%9F%E4%BD%BF%E7%94%A8%5D%0AH%20--%3E%20C%0A"})]),fallback:p(()=>E[0]||(E[0]=[a(" Loading... ")])),_:1})),E[4]||(E[4]=s("h2",{id:"mermaid-状态图-自动驾驶级别",tabindex:"-1"},[a("Mermaid 状态图：自动驾驶级别 "),s("a",{class:"header-anchor",href:"#mermaid-状态图-自动驾驶级别","aria-label":'Permalink to "Mermaid 状态图：自动驾驶级别"'},"​")],-1)),(A(),l(i,null,{default:p(()=>[e(n,{id:"mermaid-16",class:"mermaid",graph:"stateDiagram%0A%20%20%20%20%5B*%5D%20--%3E%20L0%3A%20%E9%9B%B6%E7%BA%A7(%E4%BA%BA%E5%B7%A5%E9%A9%BE%E9%A9%B6)%0A%20%20%20%20L0%20--%3E%20L1%3A%20%E4%B8%80%E7%BA%A7(%E8%BE%85%E5%8A%A9%E9%A9%BE%E9%A9%B6)%0A%20%20%20%20L1%20--%3E%20L2%3A%20%E4%BA%8C%E7%BA%A7(%E9%83%A8%E5%88%86%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6)%0A%20%20%20%20L2%20--%3E%20L3%3A%20%E4%B8%89%E7%BA%A7(%E6%9D%A1%E4%BB%B6%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6)%0A%20%20%20%20L3%20--%3E%20L4%3A%20%E5%9B%9B%E7%BA%A7(%E9%AB%98%E5%BA%A6%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6)%0A%20%20%20%20L4%20--%3E%20L5%3A%20%E4%BA%94%E7%BA%A7(%E5%AE%8C%E5%85%A8%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6)%0A"})]),fallback:p(()=>E[1]||(E[1]=[a(" Loading... ")])),_:1})),E[5]||(E[5]=s("h2",{id:"mermaid-思维导图-人工智能的局限性",tabindex:"-1"},[a("Mermaid 思维导图：人工智能的局限性 "),s("a",{class:"header-anchor",href:"#mermaid-思维导图-人工智能的局限性","aria-label":'Permalink to "Mermaid 思维导图：人工智能的局限性"'},"​")],-1)),(A(),l(i,null,{default:p(()=>[e(n,{id:"mermaid-20",class:"mermaid",graph:"mindmap%0A%20%20root((%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7))%0A%20%20%20%20%E7%A4%BE%E4%BC%9A%E9%97%AE%E9%A2%98%0A%20%20%20%20%20%20%E6%8A%80%E6%9C%AF%E6%B2%99%E6%96%87%E4%B8%BB%E4%B9%89%0A%20%20%20%20%20%20%E6%95%99%E8%82%B2%E6%A1%88%E4%BE%8B(%E6%95%99%E6%9D%90%E5%8F%91%E6%94%BE)%0A%20%20%20%20%20%20%20%20%E7%BB%8F%E8%B4%B9%E4%B8%8D%E8%B6%B3%0A%20%20%20%20%20%20%20%20%E7%AE%A1%E7%90%86%E6%B7%B7%E4%B9%B1%0A%20%20%20%20%20%20%20%20%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%A4%8D%E6%9D%82%E6%80%A7%E7%9A%84%E7%9F%9B%E7%9B%BE%0A%20%20%20%20%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%0A%20%20%20%20%20%20%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%0A%20%20%20%20%20%20%20%20%E6%84%8F%E5%A4%96%E6%83%85%E5%86%B5%E5%A4%84%E7%90%86%E8%83%BD%E5%8A%9B%E4%B8%8D%E8%B6%B3%0A%20%20%20%20%20%20%20%20%E5%AF%B9%E6%95%B0%E6%8D%AE%E7%9A%84%E4%BE%9D%E8%B5%96%0A%20%20%20%20%20%20%E9%81%93%E5%BE%B7%E9%97%AE%E9%A2%98%0A%20%20%20%20%20%20%20%20%E4%BC%A6%E7%90%86%E5%9B%B0%E5%A2%83%0A%20%20%20%20%20%20%E7%BB%8F%E6%B5%8E%E9%97%AE%E9%A2%98%0A%20%20%20%20%20%20%20%20%E6%95%B0%E6%8D%AE%E5%9E%84%E6%96%AD%0A%20%20%20%20%E5%85%AC%E5%B9%B3%E6%80%A7%E9%97%AE%E9%A2%98%0A%20%20%20%20%20%20%E7%AE%97%E6%B3%95%E6%AD%A7%E8%A7%86%0A%20%20%20%20%20%20COMPAS%E6%A1%88%E4%BE%8B%0A%20%20%20%20%20%20%E6%95%B0%E6%8D%AE%E5%81%8F%E5%B7%AE%0A%20%20%20%20%20%20%E5%85%AC%E5%B9%B3%E7%9A%84%E5%AE%9A%E4%B9%89%0A"})]),fallback:p(()=>E[2]||(E[2]=[a(" Loading... ")])),_:1})),E[6]||(E[6]=B(`<h2 id="内容总结" tabindex="-1">内容总结 <a class="header-anchor" href="#内容总结" aria-label="Permalink to &quot;内容总结&quot;">​</a></h2><div class="language-ad-think vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">ad-think</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>好的，用户提供了一篇关于梅瑞狄斯·布鲁萨德所著《人工不智能：计算机如何误解世界》的听书稿内容，需要我生成一个符合要求的回答。首先，我需要理解用户的需求。用户可能希望得到对这本书核心观点的总结，或者对书中提到的三个主要问题的深入分析。用户可能对人工智能的社会影响、自动驾驶的挑战以及算法公平性感兴趣。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>接下来，我需要确认用户身份和场景。用户可能是对人工智能话题感兴趣的普通读者，或者学生、研究者，希望快速了解这本书的主要论点。用户可能没有时间阅读全书，因此需要一个结构清晰、重点突出的摘要。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>然后，分析内容结构。原文分为作者介绍、书籍概述、三个核心部分（高估AI的社会问题、自动驾驶的难度、AI社会化应用的根本矛盾），以及总结。需要将这些内容提炼成符合格式要求的回答，使用加粗标题和分点列表，保持逻辑连贯。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>需要注意用户提供的特殊要求：使用中文，保持口语化，避免Markdown，分点叙述，每点有标题和展开。同时，要符合DeepSeek-R1的身份，开头和结尾有特定格式，中间内容自然流畅。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>需要处理的信息点包括：技术沙文主义的批评、教育系统中的教材分发案例、自动驾驶的技术与伦理挑战、COMPAS算法的公平性矛盾。要确保这些案例被准确概括，并连接到核心论点。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>潜在难点是如何将复杂的案例（如COMPAS算法中的数学矛盾）简化，同时保持准确性。需要避免技术术语，用通俗语言解释。同时，要确保各部分之间的过渡自然，整体结构符合用户的要求。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>最后，检查是否符合格式要求：没有使用Markdown，正确使用加粗符号，分点清晰，每点有标题和展开内容。确保回答在1000字左右，信息完整，逻辑严谨，语言流畅自然。</span></span></code></pre></div><p><strong>关于《人工不智能：计算机如何误解世界》的核心观点解析</strong></p><p><strong>一、高估人工智能的社会风险：技术沙文主义的困境</strong> 作者布鲁萨德批评了“技术沙文主义”——即认为技术能解决一切社会问题的盲目乐观。通过美国教材分发案例，她揭示了技术无法替代社会系统本身的复杂性：</p><ul><li><strong>数据录入的局限性</strong>：即便有中央数据库，若基层人员不配合录入或管理混乱（如校长自行使用表格），技术工具便形同虚设。</li><li><strong>标准化与个性化的矛盾</strong>：教育系统需兼顾统一标准与个体差异，但算法依赖“定义良好”的参数，难以应对动态、混乱的现实场景。</li><li><strong>技术赋能的边界</strong>：比尔·盖茨推动的“共同核心课程”因忽视地区差异和教师自主权而受阻，说明技术无法替代社会协作与制度优化。</li></ul><p><strong>二、自动驾驶的“不可能三角”：技术、伦理与经济的博弈</strong> 自动驾驶看似纯工程问题，实则暴露了AI的三大短板：</p><ol><li><strong>安全困境</strong></li></ol><ul><li>AI依赖历史数据预测常规场景，但无法处理意外（如路上突然出现的鸭子或干扰GPS信号的装置）。</li><li>传感器在雨雪天气易误判，图像识别可能混淆标志牌与广告贴纸。</li></ul><ol start="2"><li><strong>道德悖论</strong></li></ol><ul><li>面临“电车难题”时，算法需预先设定伦理选择（如奔驰优先保护乘客），但人类对机器的道德预设容忍度远低于人类自身。</li></ul><ol start="3"><li><strong>数据垄断与推广难题</strong></li></ol><ul><li>自动驾驶训练需海量地域性数据，小型公司难以与Google等巨头竞争，加剧商业不平等。</li><li>二八法则的“长尾效应”：解决最后20%的特殊路况可能需要98%的数据投入，且无法跨地区通用。</li></ul><p><strong>三、算法公平的本质矛盾：数学精确与社会正义的冲突</strong> 通过美国COMPAS再犯罪预测算法的争议，作者揭示了AI社会化应用的根本矛盾：</p><ul><li><strong>精准性与公平性的数学悖论</strong><ul><li>COMPAS算法虽剔除种族变量，但因黑人整体犯罪率高，导致黑人被误判高风险的比例（42%）是白人（22%）的近两倍。</li><li>若为“公平”调整算法降低黑人评分，则预测准确率下降，形成“保护群体”与“算法有效性”的零和博弈。</li></ul></li><li><strong>经验决策的伦理代价</strong><ul><li>AI基于历史数据决策，本质是让当前个体为群体历史行为买单（如上海学校因女生成绩更优而提高录取比例，变相歧视男生）。</li><li>分类必然伴随歧视（“丑小鸭定理”），算法无法消除社会结构性矛盾，只能反映或放大现有问题。</li></ul></li></ul><p><strong>总结：技术赋能 vs. 社会系统</strong> 布鲁萨德的批判并非否定技术价值，而是强调：</p><ul><li><strong>技术的工具属性</strong>：AI擅长优化“定义良好”的工程问题（如围棋），但社会问题需制度、文化与人协作解决。</li><li><strong>人的核心地位</strong>：从教材分发到司法算法，问题根源在于人类组织的低效与偏见，技术只是照出问题的镜子而非解药。</li><li><strong>警惕技术决定论</strong>：无人机送货、在线教育等技术方案虽局部有效，但若忽视基层执行（如超市库存系统因员工偷窃失效），仍难实现系统性变革。</li></ul><p>正如书中所言：“写程序的是人，录入数据的是人，使用系统的是人。如果不解决人的问题，技术终究不能解决真正的社会问题。”这或许是对当前AI热潮最清醒的警示。</p><h2 id="修复文案" tabindex="-1">修复文案 <a class="header-anchor" href="#修复文案" aria-label="Permalink to &quot;修复文案&quot;">​</a></h2><p>关于作者</p><p>梅瑞狄斯·布鲁萨德，毕业于哈佛大学计算机系，拥有计算机和数学的学位。她自己创造过好几个人工智能系统，在 MIT 媒体实验室工作过。布鲁萨德现在是纽约大学助理教授，同时她还是一个记者。</p><p>关于本书</p><p>你已经听过太多有关“人工智能”的话题了，什么“人工智能将会改变世界”“人工智能将让大多数人失业”……有些好消息说得太多，就容易让人产生不切实际的希望。不切实际的希望太强，就容易变成迷思。事实上，担心的那种未来离我们还非常遥远，现在所谓人工智能还没有那么厉害，人类已经反应过来了，可以预见的近期内不会有什么人工智能导致的大失业。我们这次要说的这本书，则是想让你进一步冷静一下。</p><p>核心内容</p><p>第一，高估人工智能会产生什么样的社会问题？</p><p>第二，自动驾驶这件事到底有多难？</p><p>第三，现阶段人工智能的社会化应用的根本矛盾是什么？</p><p><img src="https://piccdn3.umiwi.com/img/201902/13/201902131411067155836013.jpg" alt=""></p><p>你好，欢迎每天听本书。本期要解读的是一本2018年4月出版的英文新书《人工不智能：计算机如何误解世界》，作者是人工智能专家、数据记者和纽约大学助理教授梅瑞狄斯·布鲁萨德。如果用一句话来总结书中精髓，那就是：人工智能想大规模取代人类并不是一件容易的事情，它还有很长的一段路要走，我们对于人工智能的恐慌和其他，其实都是在过度想象。</p><p>生活在今天，相信你已经听过太多有关“人工智能”的话题，什么“人工智能将会改变世界”“人工智能将让大多数人失业”等等。但事实上，像《未来简史》那本书里担心的那种，数据和算法掌控一切的未来，离我们还非常遥远。现在的“人工智能”根本没有那么厉害，可以预见的近期内不会有什么人工智能导致的大失业。也就是说，其实我们都高估了人工智能。</p><p>电视剧《西部世界》里面那样的机器人，跟现在高科技公司正在研发的那种人工智能，完全是两码事。我们很多关于未来人工智能的种种设想，比如彻底取代人类，甚至超越人类，都只是物理学家纯理论的设想，或者干脆是科幻作家的噱头。现在真实的人工智能其实应该叫“人工不那么智能”。Alpha Go 下赢了围棋这种事情，其实没什么大不了的，它就是一个算法机器而已。</p><p>我们对于人工智能的误解，如果仅仅是观念和想象也就罢了，严重的是，因为人们过分依赖计算机算法，反而带来了一系列社会问题，所以我们需要正确认识人工智能。这也是作者梅瑞狄斯·布鲁萨德写这本书的用意。</p><p>布鲁萨德毕业于哈佛大学计算机系，拥有计算机和数学的学位。她自己创造过好几个人工智能系统，在 MIT 媒体实验室工作过。布鲁萨德现在是纽约大学助理教授，同时她还是一个记者。而且她是一种非常新型的记者，叫“数据记者”。数据记者的工作不是整天采访什么“大数据科学家”，而是自己直接从数据中挖掘故事。布鲁萨德做的事情，是自己编写一个人工智能专家系统，让这个系统替她从各种数据库中发现规律，她从中获得洞见，写成报道。</p><p>也就是说，布鲁萨德是一个“用数据发现真相”的人。布鲁萨德这个数据记者，最关心的是让那些已经在取代人类做决定的“算法”负责任。她发现人们过于相信算法。算法都是人写的，人会犯错，算法就会犯错。现在的人非常爱说什么“计算机改变了世界”，特别是乔布斯，动不动就“这个产品再一次改变了世界”。而布鲁萨德将会告诉你，其实计算机没有改变什么。社会还是这个社会，计算机并没有解决我们的社会问题。</p><p>好，接下来我就通过三个部分，来讲述本书内容。第一部分，我会让你了解，高估人工智能会产生什么样的社会问题？第二部分，我会让你明白，自动驾驶这件事到底有多难？第三部分，我会让你看清，现阶段人工智能的社会化应用的根本矛盾是什么？</p><p>我们先来说第一部分，高估人工智能会产生什么样的社会问题？</p><p>现在有一种情绪被布鲁萨德称为“技术沙文主义”，认为一切社会问题都可以用技术解决，特别是可以用计算机技术解决。布鲁萨德这本书就是专门跟技术沙文主义唱反调。布鲁萨德认为，包括人工智能在内的所谓新技术，其实并没有解决真正的社会问题。</p><p>咱们先说一个“简单”例子。假设你任职于某个贫困地区的教育局，你负责给本地所有中小学发放课本，那么你如何确保学生们都能得到课本呢？你肯定会认为这是一个简单的问题，中国最穷的地方也没发生过学生上课没有教材的事儿。但是美国有好几个州，包括纽约、宾夕法尼亚和华盛顿这样比较富的州，却有很多很多学生没有教材可用。这是一个严重的社会问题，可是却不能用技术方法解决。</p><p>几年前布鲁萨德的儿子上小学，一家人住在宾夕法尼亚州。儿子要准备全州统一的标准化考试，布鲁萨德就研究了一番考试的内容之后发现，这个考试其实可以破解的答案其实都在课本里，你只要把课本的内容参透了，通过考试就非常简单。反过来说如果你不看课本，哪怕知识面再广，也很难在考试上有好的表现。</p><p>这就是为什么美国很多老师痛恨标准化考试。但这不是我们今天想说的重点，重点在于，这么简单的考试，美国能熟练解题的学生还不到一半，宾夕法尼亚州高中的毕业率只有64%。而根本原因，就在于很多学生“拿不到课本”。</p><p>你肯定觉得这不荒唐吗？我要是中学生我第一件事就得给自己弄一套课本啊。可是对不起，真没有。咱们中国的教材是由国家统一出版，价格便宜。但美国的教材由私人公司出版，价格很贵。比如一本中学生用文学教材就要114.75美元。</p><p>政府的教育经费往往有限，有时候分到每个学生头上的教材费可能只有30多美元，这样一来就存在很大的缺口。</p><p>那你说政府钱不够我自己买一套行不行，对不起没有这个渠道。布鲁萨德她儿子的教材学校不让带回家，她想给儿子另外弄一套，各处都找不到，连盗版都没有。现在美国的中小学课本都是由三家大公司出版的，直接供货到学校，同时考试题也是这几家公司出。换句话说你是用这些公司的课本，然后用这些公司的考试题，一条龙服务，这是个每年几十亿美元的生意。</p><p>那你说能不能让学生重复使用旧教材呢？有时候的确就是这么干的，有的学生还在用八十年代师兄师姐的教材。但这个做法通常不可行，因为教材几乎每年都要随着州政府的教育政策变来变去。</p><p>这还不算，有些看似能用技术解决的问题，也解决不了。比如有时候经费到位了，书也买了，结果书被放置在学校的仓库里，没发到学生手上。老师以为今年还是没有书，校长不记得书有没有买。布鲁萨德找到一个类似于“区教委”的机构，说你们知不知道辖区内这些学校今年都开设了什么课程，需要什么教材。区教委说这个真没有，但是我们有个中央数据库，有各个学校订购教材的数据。</p><p>布鲁萨德就把这个数据库拿过来，结果一看，很多学校订购的教材数和学生人数根本对不上。布鲁萨德又跑到各个学校去现场调查，发现教材的发放和管理都非常混乱。学校经费有限，人手不够，有些校长要亲自管理教材。有的校长不愿意使用州里的数据库系统，自己弄了个表格，很难跟别人分享，最后就是一大堆问题。</p><p>布鲁萨德就说，数据技术再先进，最初的数据录入，也是由人来完成的。如果校长根本不愿意录入数据，老师根本统计不清楚自己班需要多少教材，再先进的计算机技术又有什么用呢？这里的主要问题在于，教育，本质上是个因人而异的、混乱的、动态的系统，而公共教育系统，是个统一的、标准化的、最好是不变的系统。</p><p>比尔·盖茨和梅琳达基金会一直在美国全国推行一个所谓“共同核心课程”系统，相当于中国那种全国统一的教学大纲和标准化考试，但是在各地受到了老师和学校的抵抗。其实老师的抵抗有道理，这不仅仅是不民主，而是不同学区的学生水平差异很大，用同一个标准确实不合理。学校教学没有自由度，而且应试教育的负面影响太大，结果现在美国的公共教育非常混乱。</p><p>布鲁萨德说，盖茨这帮人是想把教育当成一个工程问题去解决。工程问题本质上是数学，需要在一个定义良好的环境里，用定义良好的参数描写一个定义良好的问题。可是教育问题从来都不是“定义良好”的，其中有各种复杂的情况。复杂和标准化是一对永恒的矛盾，这就是为什么计算机技术发展了这么多年，美国基础教育几乎没有发生任何进步。</p><p>所以，我们可以看到，就在分发教材这一件事儿上，计算机人工智能都不能起到很好的作用。因为计算机和人工智能擅长的是解决工程问题，工程问题要求定义良好，而复杂社会问题恰恰是不能定义良好的。那么好，你说我们不解决社会性问题，就应对以工程问题为主的事情行不行？可能还是不行，接下来我们就来说一说，在自动驾驶，这个看上去纯工程的问题上，人工智能有多难。这是我们的第二部分内容。</p><p>我们先来说一个结论，现阶段所有的人工智能，都有一个根本的弱点。这个弱点就是它们高度依赖数据，都是对过去经验的总结，它们没有办法预测“没见过”的事情。所以这种人工智能的应用非常有限。它最适合常见的、简单的、不变的应用场景，不能直接推广。一旦遭遇必须推广的场景，就面临各种问题，比如“自动驾驶汽车”。</p><p>公众对自动驾驶汽车这个话题已经欢呼了很多年，那现在自动驾驶技术是什么情况呢？国际汽车工程师学会弄了一个标准，把自动驾驶汽车一共分为五个级别。</p><p>零级代表完全没有自动化，就是人开车。一级，是指计算机在某些时候、某种程度上可以给人提供一些辅助性的帮助。像自动刹车、保持车道、停靠辅助系等等。二级，是有的时候汽车可以自己开，但是要求人一直盯着，特斯拉已经做到了这个级别。三级，是说人可以不盯着了，就让车自己开，但是如果车向你发出信号，你要随时接管驾驶。四级，是指在某些环境和条件下，实现自动驾驶，人去睡觉都没问题。五级，是完全的自动驾驶，不论什么天气和路况人都不用管车。</p><p>截至此时此刻，任何一家公司的自动驾驶技术都没有超过二级。而有些专家认为，五级自动驾驶是一个永远都达不到的目标。这是为什么呢？因为人工智能处理不了意外。</p><p>其实你开车的时候并不是简单地把着方向盘控制着油门和刹车，你非常有智能。你要看交通信号，你要看各种路边的标志物，你要判断路上有什么东西。如果前面路上有一只小鸭子在慢慢走，你得踩刹车；但是如果是一只鸟，你可以想象车开过去它就会飞走，你就不用减速；如果路上有个塑料袋，你可以直接碾压过去；但如果那是个石头，你就必须绕着走。</p><p>你对路面状况有深刻的理解，这种理解和你的生活阅历，和你平时积累的经验有关。最起码你得知道塑料袋是什么，石头是什么，而汽车并不知道。计算机只关心这些物体的移动趋势，估算每个物体的速度，预测它的路线，看看跟车的路线会不会发生冲突。如果有冲突就踩刹车或者绕着走，但真实的路面上会有各种意外。</p><p>我们都知道，Google 一直在训练自动驾驶技术，他们遇到过各种各样奇怪的情况。有一次有几个小孩在高速公路上玩青蛙。还有一次，一个残疾人，坐着电动轮椅，在路中间追逐一只鸭子。鸭子绕圈跑，她也绕着圈追。那你说像这种情况你能一下子就准确预测这些人的行动路线吗？自动驾驶汽车识别路边的物体，都是靠把激光打到各种东西上再反射回来。可如果在下雪或者下雨，激光可能打到雪花或者雨滴上反射，汽车就可能对周围物体有重大误判。</p><p>计算机能不能保证看懂路边标记限速、慢行的交通标志牌？图形识别技术非常难，别忘了 Google 曾经把奥巴马夫人米歇尔给识别成一只猩猩。假如标志牌有损坏，或者上面被人贴了小广告，那汽车就很可能无法识别。还有，现在自动驾驶汽车都高度依赖 GPS 定位。可是现在美国有一种50美元就能买到的装置，能在周围干扰 GPS 信号。那如果路上有人使用这个装置，自动驾驶汽车要怎么办？</p><p>2016年，开特斯拉的一个司机车违反规定，把车完全交给自动驾驶，结果因为汽车没有识别出来前面的一辆白色卡车，导致死亡。它可能以为那是天上的白云或者别的什么东西。当然这是司机犯了错误，但这恰恰也说明自动驾驶技术非常容易遭遇意外。</p><p>除了安全，自动驾驶还有道德问题。比如说你正在以很快的速度开车，突然发现前边有一群小学生在马路上打闹。要避让这些小学生，你就会撞到路边的建筑物墙上；而如果撞墙，你的生命安全就面临危险。请问在这种情况下，你是选择撞墙还是选择撞向小学生呢？有道德的人，比如我，肯定是宁可自己面对生命危险，也不能撞小学生。</p><p>现在有很多公司正在研究自动驾驶的道德规范，Google 甚至还专门聘请了哲学家，但是没有研究出来什么令人满意的方案。奔驰公司已经宣布，他们对自动驾驶汽车的设定是优先保证自己车里司机和乘客的安全。在前面那种情况下，奔驰的车会果断撞向小学生。</p><p>那你说这不是杀手汽车吗？这种汽车怎么能上路呢？所以这就出现了一个道德困境。人在现场不管做出怎样的临时反应，我们都认为是正常的。可是人工智能不管事先怎么设定，我们都觉得别扭。</p><p>自动驾驶技术还有一个经济学问题。机器学习是高度依赖数据的。我们知道有个“二八法则”，说的是你花20%的时间就能解决80%的问题，剩下80%的时间解决20%的问题。对自动驾驶汽车来说，我看更可能是你用2%的数据就能训练一个能解决路面80%的情况的自动驾驶系统，但是剩下那20%的情况，你就是再用98%的数据也未必能解决。</p><p>美国50个州都有各自的交通法规，各地的气候条件和路况都不一样，这还不算美国和中国的差别。这意味着什么呢？这意味着在一个地区训练出来的自动驾驶技术，换一个地方就可能不好使。所以人工智能模型不能推广，你必须在每一个地区都采集大量的数据才行。</p><p>那好，谁拥有这么多数据呢？现在冒出来很多搞自动驾驶技术的公司，我非常怀疑他们怎么跟 Google 竞争。Google 一直都在积累数据，算法都是现成的，真正值钱的是数据。谁掌握了数据，谁的自动驾驶技术才有市场。人工智能时代的商业帝国一定是数据帝国，小的创业公司将会越来越难以起步。</p><p>所以，我们可以看到，即便是在“自动驾驶”这样看上去几乎纯工程领域的问题上，人工智能也有非常大的局限。第一，它不安全。第二，它不道德。第三，它不能促进商业平等，它只会让强大的公司变得更加强大。因此，在可以预见的未来，也许我们还是得自己开车。</p><p>这是我们说的第二部分内容，接下来我们说一说第三部分内容，也就是现阶段人工智能的社会化应用的根本矛盾是什么？</p><p>在这一部分中，我们要讲一个小道理和一个大道理。</p><p>大道理很容易理解，但是这个小道理比较烧脑，相当于是一道数学题，而且是2016年才被人想明白，还专门发表了学术论文。不过，你并不需要任何高级的数学知识，我会帮你从直观上理解这个道理。</p><p>这个事儿得从美国的司法制度说起。中国的犯人，理论上来说，不管是什么人，同样的罪行就意味着同样的刑期。在监狱里如果表现好，同样的表现等于同样的减刑。惩罚力度，跟你是个什么人，没多大关系。但是在美国，法官判决的时候，会看犯人“是个什么人”。同样的罪行，如果这个人平时表现良好，就可能得到轻判。</p><p>这个指导思想是看犯人将来再次犯罪的可能性大小。如果法官认为这个人不会再次犯罪，他就会给轻判，而且很容易批准假释。而如果法官判断这个人将来很可能再次犯罪，那就会从重从严判罚，假释减刑什么的也别想了，就在监狱里待着，省得出去危害社会。</p><p>那怎么知道一个人是什么人呢？以前都是靠法官的主观判断，肯定是不太靠谱，所以现在是用算法来判断。有个私人公司叫 NorthPointe，开发了一个算法，叫 COMPAS，专门用来判断一个犯人再次犯罪的可能性大小。COMPAS 算法会考察一个犯人的100多项指标，然后给他打一个分。从1分到10分，分数越高，代表将来再犯罪的概率就越高。</p><p>这就比法官个人的判断强多了，毕竟哪个法官也不可能同时考虑这么多项指标。算法是冰冷的，但算法也是客观冷静的。</p><p>这个算法的原理都用大数据，根据以往的经验，你考察若干个关键指标，对一件事作出判断。训练算法用的都是美国的数据，犯人都是美国的犯人。美国的犯罪环境变化也不大，这一套完全合理。而且这个 NorthPointe 公司还非常痛快，把自己用的所有数据都在网上公布了，你可以随便研究。它具体的算法是保密的，但是如果你不服，你可以自己开发一套算法跟它比。研究者发现 COMPAS 算法的准确率还真可以，算法给打了1分的犯人，再次犯罪率只有22%；而打10分的犯人再次犯罪率高达81%。</p><p>一般这种犯罪率判断，最怕被人指责种族歧视。众所周知黑人的犯罪率比较高，那有没有可能，仅仅因为犯人是黑人，你就调高他的分数呢？COMPAS 算法在这一点上做得很绝，它根本就不考虑犯人是不是黑人。100多项指标包括性别、年龄、以前的犯罪历史等等，其中根本就没有种族这个项目。事实上人们统计发现，算法给打了7分的犯人中，如果是白人，后来的再次犯罪率是60%；而7分的黑人，再次犯罪率是61%，几乎相等。所以这个算法真的没有对黑人的种族歧视。</p><p>但还是有人对 NorthPointe 公司提出了种族歧视的指控。有个专门为了社会责任搞深入新闻调查的非盈利媒体，叫 ProPublic，研究了一下 COMPAS算法。我们知道算法预测的都是概率而已，哪怕是被打了5分以上的分数，犯人也不一定会再次犯罪。如果一个犯人明明没有后来再犯罪，却被算法打了个高分，那他就等于被算法冤枉了。</p><p>ProPublic 专门统计这种没有再犯罪，却被打了5分以上的情况。同样是后来没再犯罪的犯人，黑人被打高分的可能性是42%，而白人只有22%。那这不就是说，算法还是歧视了黑人吗？这个发现引发了轩然大波。由于具体的推导过程比较复杂，所以我们只说结论。导致这个差别的原因是，黑人总体再犯罪率高，黑人打高分的比例就必然高。每个分数代表的人群中被冤枉的比例是固定的，那既然黑人总体打高分的比例高，那么其中被冤枉的黑人就一定更多。</p><p>为了方便理解，我再换个说法。假设现在有个外星人，他根本不知道地球上的人还分为黑人和白人，他以为所有人都是一样的。那么他拿到数据之后，会认为这是一套公平的打分系统。可是当我们把黑人白人的标签放进去之后，却发现黑人被冤枉的比例更高。而个别黑人之所以吃亏，是因为黑人整体的再犯罪率高。</p><p>那怎么避免这个现象呢？从数学角度根本避免不了。如果你想保护黑人，给黑人少打一些高分，那你的分数系统就是不准确的。同样一个分数就不能代表同样的再犯罪率，法官就没法从这个分数系统中获得正确的参考意见。</p><p>所以到底什么叫公平？程序员追求的是分数系统的准确性，记者要求的是不能冤枉黑人，而从数学上来说，这两个要求不可能同时满足，这是一个根本的矛盾。只要用过去的经验去预测未来，就一定有这个矛盾。产生经验的是一批人，要被预测影响的却是另外一批人。这就相当于新人要为前人犯的过错承担后果。</p><p>其实这个矛盾在生活中普遍存在。比如在上海这样，基本不存在重男轻女的大城市里，女生的考试成绩普遍比男生好。那如果根据这个规律，为了提高学校的总体成绩，在高一入学的时候尽量多录取女生，这不就是对男生的歧视吗？你的经验很可能是准确的，但是你是在用以前人的表现去惩罚后来的人。你是在让一个人为不是他自己的行为付出代价。</p><p>这是一切基于经验的决策的本质缺陷。人工智能再厉害，只要是基于经验的，只要预测不是100%准确，就一定会有人被冤枉。所谓“公平”，其实是你的主观选择。选择算法准确度的公平，你就会冤枉一些特定的黑人；选择不冤枉黑人，你的算法就不准确，你就会冤枉别的人。算法可以根本不考虑种族，但种族就隐藏在数据之中。丑小鸭定理说，一切分类都是主观的。有分类就会有歧视，此事古难全。人工智能能给我们的决策提供很大的方便，但社会还是这个社会，数学还是同样的数学，人工智能改变不了问题的本质。</p><p>好，这本书的主要内容，我们已经介绍的差不多了，下面我们来总结一下。</p><p>我们谈起技术进步，通常听到的都是好消息。比如偏边远地区不是送货难吗？我们可以用无人机送货。农村教育资源有限是吧？我们可以让农民看电视学技术啊。</p><p>技术肯定能给一部分人带来方便，但是要想全面解决一个社会问题，那可就太难了。就算是看似成功的实践，背后也不知道有多少意想不到的情况。</p><p>我在大学曾经选修过一门课叫《软件工程》。当时软件工程在中国刚刚起步，还没有多少开软件公司一夜暴富的神话，但是给我们讲课的老师当时在外面也接了一些小活儿，他有丰富的实践经验。</p><p>比如，他帮一个超市做进货、销售和存货的计算机管理系统，系统做好了，运行起来之后发现计算机显示的库存和实际总对不上账。老板就说你这个系统是不是没搞好啊？其实不是系统有问题，而是超市员工一直都在偷拿东西。在这样的情况下，你怎么还能把软件给做成和运行起来，这也是软件工程的一部分。</p><p>所以，技术的确经常能让事情更有效率，但写程序的是人，录入数据的是人，使用系统的是人。如果不解决人的问题，技术终究不能解决真正的社会问题。</p><p>撰稿：万维钢 脑图：摩西脑图工作室 转述：怀沙</p><p>1.计算机和人工智能擅长解决定义良好的工程问题，而社会问题往往不是定义良好的。 2.在自动驾驶这个看似纯工程的领域，人工智能也存在很大的局限：不安全，不道德，不能促进商业平等。 3.对于公平，人类社会存在不同的标准，人工智能无法改变社会问题的本质。</p>`,89))])}const P=o(d,[["render",m]]);export{C as __pageData,P as default};
