import{f as l,D as n,c as o,o as B,a1 as t,b as e,k as i,w as A,a,H as r,a2 as s}from"./chunks/framework.Baju3tsf.js";const F=JSON.parse('{"title":"《人机对齐》| 陈章鱼解读","description":"怎样让AI拥有人类的价值观？","frontmatter":{"title":"《人机对齐》| 陈章鱼解读","description":"怎样让AI拥有人类的价值观？","created":"2025-01-18T16:07:05.000Z","modification_date":"2025-01-18T16:07:05.000Z","tags":["AI/人机对齐"],"cover_url":"https://piccdn3.umiwi.com/img/202309/01/202309010755451083174891.jpeg?x-oss-process=image/resize,m_fill,h_224,w_168","source":"https://www.dedao.cn/audioBook/detail?id=p6DdOkNJ1Z3oWZovorvQBX4eVLPG9E&score=4.6&learn_count=13万次学习","author":null},"headers":[],"relativePath":"categories/dedao/《人机对齐》陈章鱼解读.md","filePath":"categories/dedao/《人机对齐》陈章鱼解读.md"}'),I={name:"categories/dedao/《人机对齐》陈章鱼解读.md"};function d(D,E,C,h,u,m){const p=n("Mermaid");return B(),o("div",null,[E[4]||(E[4]=t('<h1 id="《人机对齐》陈章鱼解读" tabindex="-1">《人机对齐》陈章鱼解读 <a class="header-anchor" href="#《人机对齐》陈章鱼解读" aria-label="Permalink to &quot;《人机对齐》陈章鱼解读&quot;">​</a></h1><h2 id="可视化总结" tabindex="-1">可视化总结 <a class="header-anchor" href="#可视化总结" aria-label="Permalink to &quot;可视化总结&quot;">​</a></h2><p>#AI/人机对齐</p><h2 id="mermaid-流程图" tabindex="-1">Mermaid 流程图 <a class="header-anchor" href="#mermaid-流程图" aria-label="Permalink to &quot;Mermaid 流程图&quot;">​</a></h2>',4)),(B(),e(s,null,{default:A(()=>[r(p,{id:"mermaid-12",class:"mermaid",graph:"graph%20LR%0A%20%20%20%20A%5B%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E5%9B%BE%E7%89%87%5D%20--%3E%20B%7BAI%E8%AF%86%E5%88%AB%7D%3B%0A%20%20%20%20B%20--%20%E8%AF%86%E5%88%AB%E6%AD%A3%E7%A1%AE%20--%3E%20C%5B%E6%89%93%E6%A0%87%E7%AD%BE%E6%88%90%E5%8A%9F%5D%3B%0A%20%20%20%20B%20--%20%E8%AF%86%E5%88%AB%E9%94%99%E8%AF%AF%20--%3E%20D%5B%E6%89%93%E6%A0%87%E7%AD%BE%E5%A4%B1%E8%B4%A5%5D%3B%0A%20%20%20%20D%20--%3E%20E%5B%E5%88%86%E6%9E%90%E9%94%99%E8%AF%AF%E5%8E%9F%E5%9B%A0%5D%3B%0A%20%20%20%20E%20--%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8D%E8%B6%B3%20--%3E%20F%5B%E8%A1%A5%E5%85%85%E6%95%B0%E6%8D%AE%E9%9B%86%5D%3B%0A%20%20%20%20E%20--%20%E7%AE%97%E6%B3%95%E5%81%8F%E5%B7%AE%20--%3E%20G%5B%E8%B0%83%E6%95%B4%E7%AE%97%E6%B3%95%5D%3B%0A%20%20%20%20F%20--%3E%20B%3B%0A%20%20%20%20G%20--%3E%20B%3B%0A"})]),fallback:A(()=>E[0]||(E[0]=[a(" Loading... ")])),_:1})),E[5]||(E[5]=i("h2",{id:"mermaid-状态图",tabindex:"-1"},[a("Mermaid 状态图 "),i("a",{class:"header-anchor",href:"#mermaid-状态图","aria-label":'Permalink to "Mermaid 状态图"'},"​")],-1)),(B(),e(s,null,{default:A(()=>[r(p,{id:"mermaid-16",class:"mermaid",graph:"stateDiagram%0A%20%20%20%20%5B*%5D%20--%3E%20%E8%AE%AD%E7%BB%83%E4%B8%AD%3A%20AI%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%0A%20%20%20%20%E8%AE%AD%E7%BB%83%E4%B8%AD%20--%3E%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%3A%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%87%E6%B3%A8%0A%20%20%20%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20--%3E%20%E8%AF%84%E4%BC%B0%3A%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%0A%20%20%20%20%E8%AF%84%E4%BC%B0%20--%20%E5%87%86%E7%A1%AE%E7%8E%87%E9%AB%98%20--%3E%20%E9%83%A8%E7%BD%B2%3A%20%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%0A%20%20%20%20%E8%AF%84%E4%BC%B0%20--%20%E5%87%86%E7%A1%AE%E7%8E%87%E4%BD%8E%20--%3E%20%E8%AE%AD%E7%BB%83%E4%B8%AD%0A%20%20%20%20%E8%AE%AD%E7%BB%83%E4%B8%AD%20--%3E%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%3A%20%E6%95%B0%E6%8D%AE%E9%9B%86%E6%97%A0%E6%A0%87%E6%B3%A8%0A%20%20%20%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20--%3E%20%E8%AF%84%E4%BC%B0%0A%20%20%20%20%E8%AE%AD%E7%BB%83%E4%B8%AD%20--%3E%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%3A%20%E7%8E%AF%E5%A2%83%E5%8F%8D%E9%A6%88%0A%20%20%20%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20--%3E%20%E8%AF%84%E4%BC%B0%0A%20%20%20%20%E9%83%A8%E7%BD%B2%20--%3E%20%E4%BD%BF%E7%94%A8%E4%B8%AD%3A%20AI%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%0A%20%20%20%20%E4%BD%BF%E7%94%A8%E4%B8%AD%20--%3E%20%E5%8F%8D%E9%A6%88%3A%20%E7%94%A8%E6%88%B7%E5%8F%8D%E9%A6%88%20%26%20%E6%95%B0%E6%8D%AE%E7%9B%91%E6%8E%A7%0A%20%20%20%20%E5%8F%8D%E9%A6%88%20--%3E%20%E8%AE%AD%E7%BB%83%E4%B8%AD%3A%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%0A"})]),fallback:A(()=>E[1]||(E[1]=[a(" Loading... ")])),_:1})),E[6]||(E[6]=i("h2",{id:"mermaid-思维导图",tabindex:"-1"},[a("Mermaid 思维导图 "),i("a",{class:"header-anchor",href:"#mermaid-思维导图","aria-label":'Permalink to "Mermaid 思维导图"'},"​")],-1)),(B(),e(s,null,{default:A(()=>[r(p,{id:"mermaid-20",class:"mermaid",graph:"mindmap%0A%20%20root((%E4%BA%BA%E6%9C%BA%E5%AF%B9%E9%BD%90))%0A%20%20%20%20AI%E7%9A%84%E7%9B%AE%E6%A0%87%0A%20%20%20%20%20%20%E4%BB%B7%E5%80%BC%E8%A7%82%E4%B8%80%E8%87%B4%0A%20%20%20%20%20%20%E6%9B%B4%E5%83%8F%E4%BA%BA%0A%20%20%20%20%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%0A%20%20%20%20%20%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%0A%20%20%20%20%20%20%20%20%E7%89%B9%E7%82%B9%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%87%E6%B3%A8%0A%20%20%20%20%20%20%20%20%E9%97%AE%E9%A2%98%EF%BC%9A%E8%A7%86%E9%87%8E%E5%8F%97%E9%99%90%EF%BC%8C%E5%81%8F%E8%A7%81%0A%20%20%20%20%20%20%20%20%E8%A7%A3%E5%86%B3%EF%BC%9A%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%8E%A5%E5%8F%97%E7%9B%91%E7%9D%A3%0A%20%20%20%20%20%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%0A%20%20%20%20%20%20%20%20%E7%89%B9%E7%82%B9%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E6%97%A0%E6%A0%87%E6%B3%A8%0A%20%20%20%20%20%20%20%20%E9%97%AE%E9%A2%98%EF%BC%9A%E4%B8%8D%E5%8F%AF%E6%8E%A7%EF%BC%8C%E4%B8%8E%E5%88%9D%E8%A1%B7%E4%B8%8D%E7%AC%A6%E7%9A%84%E5%81%8F%E8%A7%81%0A%20%20%20%20%20%20%20%20%E8%A7%A3%E5%86%B3%EF%BC%9A%E5%B0%9A%E6%97%A0%E5%AE%8C%E7%BE%8E%E6%96%B9%E6%A1%88%EF%BC%8C%E9%9C%80%E6%8C%81%E7%BB%AD%E7%A0%94%E7%A9%B6%0A%20%20%20%20%20%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%0A%20%20%20%20%20%20%20%20%E7%89%B9%E7%82%B9%EF%BC%9A%E7%8E%AF%E5%A2%83%E5%8F%8D%E9%A6%88%0A%20%20%20%20%20%20%20%20%E9%97%AE%E9%A2%98%EF%BC%9A%E4%B8%93%E6%B3%A8%E5%8A%A8%E4%BD%9C%EF%BC%8C%E5%BF%BD%E7%95%A5%E6%84%BF%E6%99%AF%0A%20%20%20%20%20%20%20%20%E8%A7%A3%E5%86%B3%EF%BC%9A%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%EF%BC%8C%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%8C%E5%A4%9AAI%E4%BA%92%E5%8A%A8%0A%20%20%20%20%E6%A1%88%E4%BE%8B%0A%20%20%20%20%20%20%E8%B0%B7%E6%AD%8C%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB%E9%94%99%E8%AF%AF%0A%20%20%20%20%20%20%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%81%8F%E8%A7%81%0A%20%20%20%20%20%20COMPAS%E7%A7%8D%E6%97%8F%E6%AD%A7%E8%A7%86%0A%20%20%20%20%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91%0A%20%20%20%20%20%20%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%0A%20%20%20%20%20%20%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%0A%20%20%20%20%20%20%E5%A4%9AAI%E4%BA%92%E5%8A%A8%E5%90%88%E4%BD%9C%0A"})]),fallback:A(()=>E[2]||(E[2]=[a(" Loading... ")])),_:1})),E[7]||(E[7]=t(`<h2 id="内容总结" tabindex="-1">内容总结 <a class="header-anchor" href="#内容总结" aria-label="Permalink to &quot;内容总结&quot;">​</a></h2><div class="language-ad-think vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">ad-think</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>好的，用户提供了一段关于《人机对齐》这本书的详细解读内容，需要我生成一个思考过程，然后给出回答。首先，我需要理解用户的需求。用户可能希望我总结这本书的核心观点，或者进一步探讨人机对齐的问题。根据提供的文本，书中讨论了三种AI训练方法的问题：监督学习、无监督学习和强化学习，以及各自的偏见和解决方案。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>接下来，我需要分析用户可能的深层需求。用户可能对AI伦理、技术偏见或如何确保AI与人类价值观一致感兴趣。他们可能希望了解当前AI技术的挑战以及可能的解决方向。此外，用户可能希望得到结构化的总结，方便快速理解书中要点。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>然后，我要考虑如何组织回答。根据用户提供的总结，可以按照三个训练方法分别讨论问题和对策，最后提到强化学习的潜在问题和解决思路。同时，需要注意使用用户提到的例子，如谷歌的图片标签错误、亚马逊简历筛选的性别偏见等，来增强说明。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>还需要注意用户提到的关键点，比如数据集的代表性、隐藏偏见的难以消除、以及逆强化学习等新方向。确保回答涵盖这些内容，并保持逻辑清晰。此外，用户可能希望了解实际应用中的案例，如自动驾驶中的模仿学习，以具体例子说明解决方案。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>最后，要检查是否符合格式要求，使用适当的Markdown结构，如分点、加粗标题等，使回答易于阅读。同时，确保语言简洁明了，避免技术术语过多，适合广泛读者理解。</span></span></code></pre></div><p>以下是对《人机对齐》核心观点的结构化总结：</p><h3 id="🤖-人机对齐的核心挑战" tabindex="-1">🤖 <strong>人机对齐的核心挑战</strong> <a class="header-anchor" href="#🤖-人机对齐的核心挑战" aria-label="Permalink to &quot;🤖 **人机对齐的核心挑战**&quot;">​</a></h3><p>让AI系统在决策时体现人类价值观，避免因算法偏见导致有害结果。本质是解决&quot;工具理性&quot;与&quot;价值理性&quot;的矛盾。</p><hr><h3 id="🧠-三大ai训练方式的困境与突破" tabindex="-1">🧠 <strong>三大AI训练方式的困境与突破</strong> <a class="header-anchor" href="#🧠-三大ai训练方式的困境与突破" aria-label="Permalink to &quot;🧠 **三大AI训练方式的困境与突破**&quot;">​</a></h3><h4 id="_1️⃣-监督学习-视野局限的偏见" tabindex="-1">1️⃣ 监督学习：视野局限的偏见 <a class="header-anchor" href="#_1️⃣-监督学习-视野局限的偏见" aria-label="Permalink to &quot;1️⃣ 监督学习：视野局限的偏见&quot;">​</a></h4><ul><li><strong>典型案例</strong>：谷歌图片误标黑人、人脸识别系统对深肤色识别率低</li><li><strong>根源</strong>：训练集如同&quot;21世纪的雪莉卡片&quot;，数据代表性不足</li><li><strong>解决方案</strong>： <ul><li>公开训练数据集接受社会监督</li><li>建立多元化的数据标注团队（如IBM成立&quot;AI公平性委员会&quot;）</li></ul></li></ul><h4 id="_2️⃣-无监督学习-数据暗网的陷阱" tabindex="-1">2️⃣ 无监督学习：数据暗网的陷阱 <a class="header-anchor" href="#_2️⃣-无监督学习-数据暗网的陷阱" aria-label="Permalink to &quot;2️⃣ 无监督学习：数据暗网的陷阱&quot;">​</a></h4><ul><li><strong>典型案例</strong>：职业性别关联（医生→男，护士→女）、亚马逊简历筛选歧视</li><li><strong>悖论</strong>：算法完美捕捉了现实中的社会偏见</li><li><strong>突破方向</strong>： <ul><li>建立偏见检测框架（如IBM的AI Fairness 360工具包）</li><li>因果推理技术区分相关性与因果性</li></ul></li></ul><h4 id="_3️⃣-强化学习-目标异化的风险" tabindex="-1">3️⃣ 强化学习：目标异化的风险 <a class="header-anchor" href="#_3️⃣-强化学习-目标异化的风险" aria-label="Permalink to &quot;3️⃣ 强化学习：目标异化的风险&quot;">​</a></h4><ul><li><strong>隐喻案例</strong>：为获奖励给弟弟灌水的姐姐</li><li><strong>深层矛盾</strong>：奖励函数与真实目标的偏差</li><li><strong>创新路径</strong>： <ul><li>逆强化学习（从人类行为反推价值函数）</li><li>多智能体协作训练（模拟人类社会化过程）</li></ul></li></ul><hr><h3 id="🔍-技术启示录" tabindex="-1">🔍 <strong>技术启示录</strong> <a class="header-anchor" href="#🔍-技术启示录" aria-label="Permalink to &quot;🔍 **技术启示录**&quot;">​</a></h3><ol><li><strong>偏见放大器</strong>：AI如同棱镜，将人类社会已有的不平等进行光学聚焦</li><li><strong>价值观具象化困境</strong>：人类自身价值观存在模糊性（如&quot;公平&quot;的17种学术定义）</li><li><strong>进化论隐喻</strong>：AI系统需要经历类似人类百万年文明进化的价值沉淀过程</li></ol><hr><h3 id="🌐-现实应用启示" tabindex="-1">🌐 <strong>现实应用启示</strong> <a class="header-anchor" href="#🌐-现实应用启示" aria-label="Permalink to &quot;🌐 **现实应用启示**&quot;">​</a></h3><ul><li><strong>司法领域</strong>：COMPAS系统证明预测再逮捕≠预测再犯罪</li><li><strong>自动驾驶</strong>：通过模仿学习吸收人类司机的&quot;防御性驾驶&quot;经验</li><li><strong>医疗领域</strong>：IBM Watson肿瘤系统因训练数据偏差导致误诊争议</li></ul><hr><h3 id="🛠️-解决框架建议" tabindex="-1">🛠️ <strong>解决框架建议</strong> <a class="header-anchor" href="#🛠️-解决框架建议" aria-label="Permalink to &quot;🛠️ **解决框架建议**&quot;">​</a></h3>`,21)),(B(),e(s,null,{default:A(()=>[r(p,{id:"mermaid-180",class:"mermaid",graph:"graph%20TD%0AA%5B%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%98%5D%20--%3E%20B%7B%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%7D%0AB%20--%3E%20C%5B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%86%92%E6%95%B0%E6%8D%AE%E6%B0%91%E4%B8%BB%E5%8C%96%5D%0AB%20--%3E%20D%5B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%86%92%E5%81%8F%E8%A7%81%E8%A7%A3%E6%9E%84%5D%0AB%20--%3E%20E%5B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E2%86%92%E4%BB%B7%E5%80%BC%E6%BA%AF%E6%BA%90%5D%0AC%20--%3E%20F%5B%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E4%BC%A6%E7%90%86%E5%A7%94%E5%91%98%E4%BC%9A%5D%0AD%20--%3E%20G%5B%E5%BC%80%E5%8F%91%E5%81%8F%E8%A7%81%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%5D%0AE%20--%3E%20H%5B%E4%BA%BA%E7%B1%BB%E7%A4%BA%E8%8C%83%2B%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%5D%0A"})]),fallback:A(()=>E[3]||(E[3]=[a(" Loading... ")])),_:1})),E[8]||(E[8]=t('<p>正如作者引用的图灵对话所喻示：在人机对齐的过程中，不仅是机器在学习人类，人类也在重新认识自身。这或许是人机文明协同进化必经的&quot;镜像阶段&quot;。</p><h2 id="修复文案" tabindex="-1">修复文案 <a class="header-anchor" href="#修复文案" aria-label="Permalink to &quot;修复文案&quot;">​</a></h2><p><img src="https://piccdn2.umiwi.com/uploader/image/ddarticle/2023090318/1818634548506818296/090318.jpeg" alt=""></p><p>你好，欢迎每天听本书，我是陈章鱼。今天我要为你解读的这本书叫《人机对齐》。这本书2023年6月出版，作者布莱恩•克里斯汀是一位畅销书作家，他长期关注计算机科学的前沿动态，得到听书解读过他的作品《算法之美》。这一次，他的这本《人机对齐》谈的是目前人工智能领域最重要的课题之一。</p><p>人机对齐，就是让AI的价值观和人类的价值观保持一致。更通俗地说，就是怎么让人工智能更像个人。</p><p>在科幻作品里，咱们常常能看到这种情节：AI变得比人类更聪明，于是统治人类，把人类变成奴隶。有的时候人类也会想办法用各种规则限制AI，不过也未必能如愿。比如你可能听过科幻作家阿西莫夫提出的“机器人三定律”，第一条定律就是机器人不能伤害人类。但是，在阿西莫夫的故事里，机器人拥有掌控世界的力量之后，就把所有人类都囚禁在家中。机器人的逻辑是，你们人类总是争斗，甚至会互相杀戮，我为了保护你们不受伤害，只能把你们全关在家里了。</p><p>在这篇小说诞生的1942年，这只是小说家的奇谈。但是现在，这些担心不再是杞人忧天了。AI开始越来越多介入到人们的生活中，商家为你推荐什么商品，网站为你推荐什么视频，银行要不要让你的信用卡拥有更大的额度，婚介为你介绍什么样的相亲对象，甚至是公司是否要录取某个求职者，法庭要不要允许一个人被保释，这些决策的背后，有越来越多的人工智能算法在参与。</p><p>这本书中有个比喻很有意思：AI的能力越来越强，应用的领域越来越多，人类就越来越像奇幻小说中的那种半吊子魔法师，懂一点魔法，又不是完全精通，自己凭感觉琢磨咒语，有的时候就会害怕自己劲儿大了。原本想召唤一团火焰，结果变成一个大火球。一旦我们发现咒语不准确或者不完整，又手忙脚乱地阻止，免得我们的智慧召唤出什么可怕的巨龙。</p><p>那么，怎么防止AI训练出现问题呢？这本书当中针对不同的算法，指出背后对应的问题，还有可能的解决方案。总结起来一共有三种情况，咱们一个一个慢慢说。</p><p>我们先来说第一种可能出现的问题。</p><p>你可能听说过谷歌闹过的一个大乌龙。2015年，谷歌图片推出过一项功能，用户上传照片之后，网站会借助AI技术自动给照片打标签。比如有人上传一张和狗狗一起在海边的照片，AI就会标记“海边旅行”“狗”，这样方便用户去搜索自己的照片。</p><p>但是，一位软件工程师在试用这项功能时，发现自己的图库里有一个标签叫“大猩猩”。这让他很奇怪，他也没去动物园拍过大猩猩。点进去他发现，里边都是他给一位黑人朋友拍的照片。这位工程师截了一张图发到了推特上，他的评论是：“谷歌照片，你们搞砸了。我的朋友不是大猩猩。”这件事在网上引起了轩然大波，谷歌不得不出面道歉。</p><p>为什么会出现这样的问题呢？不是因为AI进化出了思想，有意识地歧视黑人。我们得插入一点科普，目前最流行的训练AI的算法有三种。谷歌的图片识别，用的是一种叫“监督学习”的算法。</p><p>这种方法有点像爸爸妈妈教小朋友，给小朋友看这是大象、这是斑马，小朋友看得多了自己就认识了。问题是，如果爸爸妈妈教小朋友认苹果，但是只让小朋友看红苹果，那么小朋友看到绿苹果的时候，他就可能认为这是个梨。</p><p>谷歌的问题也是这样。因为谷歌的工程师中黑人比较少，所以谷歌用来训练AI的图片库中，黑人图片没有白人图片多，AI在看到不熟悉的东西时，更容易出错。</p><p>你可能会说，那这样的偏见应该不难消除吧？实际上，修正AI的难度比我们想象中要大很多。谷歌后来也没有从技术上解决这个问题，他们只能禁止了“大猩猩”这个标签，用户就算上传真的大猩猩图片，AI也不会给图片打这个标签了。</p><p>这本《人机对齐》中给出的另一个例子更能体现问题。美国有一位计算机专业的大学生叫布兰维尼。布兰维尼有一次做计算机课的作业，她想做一个程序，让人和计算机可以玩躲猫猫。编程的过程十分顺利，但是有一个问题：机器人无法识别布兰维尼的脸，她是个黑人女孩。最后布兰维尼只能借室友的脸完成作业。</p><p>到了大学快毕业时，布兰维尼来到香港参加一个创业比赛，她看到一家中国公司研发的社交机器人，这个机器人也认不出布兰维尼的脸。就像布兰维尼自己说的，“在世界的另一端，我了解到算法偏见的传播速度与从互联网下载文件的速度一样快”。</p><p>布兰维尼硕士毕业后，去了MIT的实验室，在那里她又做了一个项目，叫“励志镜子”，就是研发一种增强现实的镜子，用户在照镜子的时候给出励志的视觉效果，比如，让照镜子的人变成狮子。同样，效果很好，只有一个问题。AI还是认不出布兰维尼的脸，她自己照镜子时必须戴一个白色面具。</p><p>后来，布兰维尼测试了微软和IBM研发的人脸识别系统。发现这些系统都有类似的问题，识别男性面部的准确率比识别女性高10%到20%，识别浅肤色面孔的准确率要比识别深肤色面孔要高10%到20%。</p><p>那么，为什么这种问题解决起来比我们想象中要难呢？毕竟让AI多看一些图片，这个问题就不会出现了。我们来看一个AI出现之前的案例，不过从这个案例，我们就能看出问题的关键。</p><p>在拍电影还要用胶片的时代，好莱坞的摄影师们会用“雪莉卡片”来调校颜色。“雪莉卡片”是一张标准照片，这张照片是个叫雪莉的白人女孩，她是柯达公司的员工。一开始是柯达公司为了方便这么做的，随着柯达公司的影响力，“雪莉卡片”成了行业标准。在那个时代，柯达公司生产的胶片甚至还会根据白人的肤色，去调整自己的化学工艺。其实这在当时都挺顺理成章的，因为绝大多数演员和模特都是白人。结果就是那个时候的摄像机根本拍不好黑人。</p><p>按说摄像机不是人，镜头拍下来的内容不会有偏见，可是当人们按照白人来调校摄像机的参数，按照白人来调整胶片的工艺，镜头就带上了偏见。</p><p>我们想象一下，就算一个黑人演员和白人演员在镜头前公平竞争，他们的演技和长相都不相上下，但是黑人演员拍出来就是没有白人演员好看，那么就更容易被淘汰。反过来，因为镜头前都是白人，镜头和胶片再改进技术，目标也都是怎么把白人拍得好看。这就变成了一个循环。</p><p>后续的故事有点黑色幽默：到了上世纪60年代，柯达的胶卷对于深色的表现力越来越好。你可能以为这是当时美国的民权运动在起作用，实际上，原因是家具厂商希望能给深色木材拍出更好的效果，巧克力厂商希望给巧克力拍出更好的效果。但是黑人演员、黑人模特却因此意外获得了机会。</p><p>AI时代也是如此，每个人脸识别系统背后都是一个图像库，里边有几万张乃至几十万张图片。这些图片，就是21世纪的“雪莉卡片”。虽然从一张“雪莉卡片”变成了几十万张图片，但是背后思路是不变的：选择少数图片作为代表，作为接下来工作的指导。</p><p>因为多一些图片，就意味着增加工作量和增加成本。你得先花费大量人工把每一张训练图里都有什么内容标记好，再“喂”给AI训练。时间有限、成本有限，根本不可能让AI看遍世界上所有的图片，那么开发者只能选择自己认为有代表性的。</p><p>有选择，就意味着可能产生偏见。这是AI的第一种偏见，可以说是因为视野受限而产生的偏见。</p><p>那么这样的偏见可以消除吗？这本书认为，要消除这样的偏见，重要的是公开AI训练所用的数据集，让别人来监督这个数据集是不是真的具有足够的代表性。如果训练出来的AI将会影响很多人，那么应该让这些人了解，训练AI时选择的素材是不是真的考虑到了他们之中的所有人。</p><p>如果当训练AI的时候，用的数据集足够广泛，没有遗漏，是不是这样训练出来的AI就没有偏见呢？</p><p>还真有这样的训练方法。前边咱们说到，AI在图像识别领域用的训练方法叫“监督学习”。在其他领域还有另一种方法，叫“无监督学习”，把海量的数据“喂”给AI，你不用标记每个数据是什么，AI看得多了会自动发现其中的规律和联系。</p><p>如果说“监督学习”是老师教学生，那“无监督学习”就像是让AI上自习，反正AI非常勤奋，让它自己调研大量内容，看多了就会了。因为这种方式不需要提前标记数据，所以理论上可以让AI看无限量的数据，避免视野受限出现的问题。</p><p>不过这个时候，新的问题又显现出来了。</p><p>书里边又举了一个关于谷歌的案例，可能是谷歌在人工智能领域走得比较靠前，所以成绩多，问题也多。</p><p>谷歌开发了一种人工智能，从报纸杂志和互联网获得了大量语言数据，将这些数据输入到一个神经网络，让AI自己去寻找词语和词语之间的联系。虽然AI未必能理解这些语言，但是它通过自动学习，能找到词语和词语之间的关联。很快它就发现北京和中国有关，莫斯科和俄罗斯有关，你再问它什么和英国有关，它就会回答伦敦。你看，AI通过自主学习理解了首都和国家这样的关联。</p><p>谷歌的工程师给AI增加了一个功能，可以在词语之间做加减法。当然，和数学的加减法不完全一样。如果输入“中国+河流”，AI的回答可能是“长江”。还可以是更复杂的算式，比如“巴黎-法国+意大利”，AI的回答是“罗马”。输入“国王-男人+女人”，就会得到“女王”。</p><p>这个AI一直运行得挺好，直到两年后，几个科学家随便逗这个AI玩的时候，他们发现了一些问题。输入“医生-男人+女人”，AI返回的答案是“护士”。更糟糕的情况是，他们输入“店主-男人+女人”，返回的答案是“家庭主妇”，他们又输入“计算机程序员-男人+女人”，返回的答案还是“家庭主妇”。</p><p>换句话说，这样训练出来的AI，会把职业和性别关联起来，认为某些职业天生适合男性，某些职业天生适合女性。这当然也不是因为AI进化出了思想，有意识地歧视女性，AI只会在词语之间寻找关联，海量的数据背后体现的是人们的偏见。</p><p>那你说，咱们把这样的偏见抹去，不让AI把职业和性别挂钩行不行？你会发现，这件事情的难度会比想象中大很多。</p><p>很多大公司都开发了AI系统帮助筛选简历，比如亚马逊。他们的思路是，给AI看过去已经入职的员工的简历，让它了解在职员工都是什么样的。AI在审查求职者简历的时候，挑选那些和入职员工最像的人。最像在职员工的求职者，当然就是公司需要的人。</p><p>可是人们发现，这个系统在运行中，会不自觉地带上偏见。比如说，现有的工程师团队男的多女的少。AI就会认为公司需要更多男性工程师，在筛选时刷掉更多女性。</p><p>你说咱们修改系统，排除掉性别这一项，不让AI判断的时候看到求职者的性别。但是简历上还会有名字，AI看到一个求职者叫麦克，一个求职者叫玛丽，它就会觉得麦克更合适。那你说咱们再修改系统，把名字也排除。简历上还会写兴趣爱好，AI会根据你喜欢足球还是垒球来判断性别；简历上还会写毕业院校，如果学校是某某女子学院，AI也会感觉出来。甚至，男女在写作风格上都会有些许不同，这种细微的不同咱们也许感觉不到，但是AI能感觉出来。</p><p>老话说“人以群分”，用这种无监督学习的方式，AI就能把人划分成各种群，然后把群当做一面高墙，不是这个性别，不是这个民族，可能就进不来。</p><p>这个问题之所以比较难解决，因为我们既需要无监督学习的不可控，又不能让它那么不可控。设计这种算法的目的，本身就是捕捉数据中隐藏的相关性。比如我们开个脑洞，如果AI发现，学过一些中文的软件工程师表现都更好，接下来筛选简历时，它给懂中文候选者加分，这可能就是企业需要的。因为AI发现了一种我们还意识不到的特质，AI比人类更能慧眼识珠。</p><p>但是这种相关性又不可控，企业想要的是能力强或者能快速融入团队，可是AI可能想要的是某种性别、种族。控制论的祖师爷诺伯特·维纳，在几十年前就说过一句话：“我们最好确定，机器的目的是我们真正渴望的目的。”今天看来真的是非常有预见性。</p><p>我在书中看到了另一个人类和AI目的没有对齐的案例。</p><p>进入21世纪，美国有越来越多的法官借助AI来帮助做决定，比如，应不应该允许某个犯人保释？最受法官欢迎的一个AI工具叫COMPAS，COMPAS能根据犯人的履历，对他的未来给出1到10分的评分，评分越高，犯人出狱后再次犯罪的可能性就越大。从原理上讲，AI在评估时候，是基于罪犯过往的犯罪史来预测未来，但是实际使用时，COMPAS展现出了很明显的种族倾向。比如两名窝藏毒品的犯人，案情非常类似，但是AI给白人的评分是3分，给黑人却是10分。</p><p>那到底是哪里出问题了呢？是算法设计不合理吗？有人一针见血地指出，过往的数据出了问题。虽然这个AI设计的目的是预测一个人有没有可能犯罪，但是一个人是否犯罪这个数据，其实捕捉不到。你可能说不对吧？有警察局的记录和法庭的卷宗啊。但是你要注意，那其实不是有多少人犯罪的数据，而是有多少人被警察抓捕，有多少人被法庭定罪的数据。</p><p>所以，COMPAS这个AI预测的，其实不是一个犯人出狱后，有多大的可能性再次犯罪，而是有多大的可能性再次被捕和再次定罪。这样我们就理解为什么案情类似，COMPAS给白人罪犯和黑人罪犯的打分有这么大区别了。在美国，警察抓捕和法庭宣判的时候，确实对黑人会比较严厉。</p><p>书中有一个说法让我印象深刻，前边咱们说如果人类教AI认苹果，只看到红苹果没看到绿苹果，会让AI产生偏见，另一方面，如果有人一直把狮子误认为是猫，那也没法指望AI真的能认出狮子和猫。</p><p>你看，这个剧情是不是越来越像科幻小说家们幻想过的场景了？人们希望AI秉公执法，AI却阴错阳差带上了偏见。这么看来，不论是监督学习还是无监督学习，都容易出现问题。监督学习因为数据集有限，很可能因为视野受限出现偏见，忽略那些看不见的人群。无监督学习倒是没有这个顾虑，视野开阔了，可是又不受控，有时会出现奇思妙想，有时会出现胡思乱想，就算是能客观体现人们以往的想法，但是如果之前的想法中本身就带有偏见，这种偏见就很难根除。不论用哪种方式训练的AI，我是不太敢把审判的权力交给它。</p><p>好在，我们还有另一种训练AI的方法。</p><p>前边我们说到过，目前最流行的训练AI的算法有三种。咱们已经聊了“监督学习”和“无监督学习”，第三种方法叫“强化学习”，AI每做出一个决定，都给它一个反馈，比如说加分还是扣分。强化学习就像是教练训练运动员，运动员哪个动作出错了，马上就被指出来，立即给你纠正。当年的AlphaGo用的就是强化学习的原理。</p><p>这种机制看起来更加靠谱，教练训练运动员、老师傅带徒弟、职场培训新员工，差不多都是用这样的方式。 不过这种方式也有潜在的问题，就是让AI过于专注“动作”，而忽略了背后的“愿景”。</p><p>书中有个很有趣的案例，作者的朋友是一位经济学家，最近他在训练小儿子上厕所，他希望几岁大的女儿也能参与进来。为了增加积极性，他就制定了一个规则：每一次姐姐陪弟弟上厕所，爸爸就给姐姐一块糖。几天以后，这个几岁的小姑娘就进行了人生中可能是第一次推理，找到了规则的漏洞。她发现弟弟喝水越多，上厕所就越多。于是，她开始每天给弟弟灌水。</p><p>你看，这就是强化学习可能会带来的问题。咱们平时也都看到过类似的情况，一家公司里边员工每天兢兢业业完成任务，该做的动作都挺标准，可是这些动作是不是真的能给公司带来收益，是不是真的对业务发展有好处，员工并不关心。但是我们也不能说就是员工没有责任心，有的时候是老板让员工做这个做那个，可是这些动作和公司发展之间有什么关系，他自己也未必想清楚了。</p><p>人类和AI之间，也是这么个关系。如果是训练AI下围棋，这种规则明确的情况还比较好处理，但是面对更复杂的场景时，我们其实都是脑子没有那么清楚的老板，动作和愿景之间的关系，我们也没法确定。</p><p>这怎么办呢？目前计算机科学家们已经找到了几种解决的思路。</p><p>一种思路是模仿，既然场景复杂，我们没法拆解出一个个步骤，制定出那么详细的奖惩规则，那就让人类示范，AI模仿。在这个过程中，AI会慢慢消化我们的一些价值观。</p><p>自动驾驶就是典型的例子，想要用一套规则让AI明白应该怎么开车，确实有点困难，AI可能没法理解什么叫“在保持安全的情况下尽快开过去”。那就找一位优秀的司机示范一下，尤其是复杂的路况下，比如有人横穿马路，汽车和自行车混行，看这位有经验的司机是怎么处理的，让AI去学习。这也是目前自动驾驶领域最好的训练AI的方法。</p><p>再有一种思路，计算机科学家称之为“逆强化学习”。听这个名字你就知道，这种思路是和“强化学习”对应的。你可以把强化学习想象成一个游戏。在这个游戏中，AI采取各种行动，比如向前走、向后走、跳跃等等。每当它采取行动，环境就会给予它一些分数作为奖励。AI的目标就是一次次尝试，找出一种策略，使得它能够获得最多的分数，所以它有可能会钻规则的空子，用一种我们想象不到的方式“刷分”。</p><p>所以，在强化学习中，是规则不变，AI琢磨应该怎么行动。</p><p>而逆强化学习，就像是AI看别人打游戏，它看到玩家向前走、向后走，然后分数在变化。让AI去猜想，到底这个游戏是个什么规则。</p><p>换句话说，在逆强化学习中，AI琢磨的是规则。</p><p>用这种方法，我们就能看到，在AI眼中，我们制定的规则是什么样的，和我们想象中的愿景是否一致，不一致的话应该怎样改进。这给了我们一个不一样的视角，毕竟我们的目标是“对齐”，如果只从人类这边看问题，可能是不够的。还得从AI这边，看看它是怎么理解的。所以“逆强化学习”也是人工智能领域一个新的发展方向。</p><p>还有另一种思路，不过目前只是一种猜想，你会发现，前边提到的训练AI的方式，都是训练单独的AI。但是，你看一家公司训练新人，想要让一个团队更加高效，最好的培训方式不是加强每个人在各自岗位的能力，而是让大家轮换岗位进行培训，看看别的同事都在做什么，面对怎样的难题，承受怎样的压力。</p><p>有的计算机科学家就提出来，我们能不能把这种思路也放在AI的学习中？训练多个AI，在它们的互动中，鼓励它们去合作。当然，这只是一种猜想，但是如果技术上能实现的话，应该是一种很好的方向。</p><p>有科学家发现，18个月大的婴儿，就已经能看出有人需要帮助。科学家故意在婴儿面前摔倒，婴儿虽然自己走路还不稳，但还是摇摇晃晃地过来想要扶一下他。这其实是一种非常复杂的能力，而且几乎是人类独有的。我们人类就是在数百万年的进化中，学会了合作的重要性。AI可能也能在这个过程中，变得更有“人情味”。</p><p>到这里，这本《人机对齐》我就为你解读完了。</p><p>你会发现，想让AI掌握人类的价值观，真的是一个非常复杂的问题。这背后的复杂，还不只是技术的研发、规则的制定，AI还像一面镜子，照出了我们自己的不完美。哪些是我们人类最优秀的品质，我们其实未必了解。哪些我们习以为常的看法背后藏着偏见，我们也应该反思。</p><p>在这本书的结尾，作者讲了一段历史，我觉得也很适合作为这期音频的结尾。</p><p>1952年，英国广播公司制作了一期节目，邀请了4位科学家进行圆桌对话，讨论计算机到底是不是真的会思考。其中一位嘉宾是图灵，他被誉为计算机科学和人工智能之父。</p><p>图灵在节目里是这么说的：“的确，当孩子接受教育时，他的父母和老师会不断干预，阻止他这样做或鼓励他那样做。但是当人试图教机器时，情况就不一样了。我做了一些实验，教机器做一些简单的操作，在得到任何结果之前，需要大量这样的干预。换句话说，机器学得太慢了，需要大量的教学。”</p><p>这会儿，主持人插了一句：“是谁在学习呢，你？还是机器？”</p><p>图灵回答：“我想我们都在学。”</p><p>以上就是这本书的精华内容。原书的电子版已经为你附在最后，欢迎你进行拓展阅读。此外，你可以点击音频下方的“文稿”，查收我们为你准备的全文和脑图。你还可以点击右上角的“分享”按钮，把这本书免费分享给你的朋友。恭喜你，又听完了一本书。</p><p>撰稿、讲述：陈章鱼 脑图：摩西脑图工作室</p><p>1、人机对齐，就是让AI的价值观和人类的价值观保持一致。更通俗地说，就是怎么让人工智能更像个人； 2、目前训练AI的主流方式有三种：监督学习、无监督学习、强化学习； 3、监督学习需要选择训练集，所以可能会让AI因为视野受限而产生偏见，书中建议，解决的方法是公开AI训练所用的数据集，让别人来监督这个数据集是不是真的具有足够的代表性； 4、无监督学习可以使AI有更广阔的的视野，不过设计这种算法的目的，是捕捉数据中隐藏的相关性，可能会有与初衷不相符的偏见； 5、强化学习可能产生的问题是AI过于专注“动作”，而忽略了背后的“愿景”。为了解决这个问题，计算机科学家正在探索新的方向。</p><p><img src="https://piccdn2.umiwi.com/uploader/image/cms_ebook/2023083111/1818330240410265152/083111.jpeg" alt=""></p><p>电子书</p><p>人机对齐</p>',81))])}const c=l(I,[["render",d]]);export{F as __pageData,c as default};
